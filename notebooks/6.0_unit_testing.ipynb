{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the main directory\n",
    "# So, it's executed from main directory\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.env') as f:\n",
    "    os.environ.update(\n",
    "        line.strip().split('=') for line in f\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Testing Config\n",
    "\n",
    "This code will be apply in `src/MarketplaceReviews/entity/config_entity.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class UnitTestConfig:\n",
    "    root_dir: Path\n",
    "    mlflow_tracking_uri: str\n",
    "    mlflow_model_name: str\n",
    "    mlflow_deploy_model_alias: str\n",
    "    mlflow_input_example_path: Path\n",
    "    app_endpoint: str\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Testing Config Manager\n",
    "\n",
    "This code will be apply in `src/MarketplaceReviews/config/configurations.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MarketplaceReviews.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "from MarketplaceReviews.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, \n",
    "                 config_filepath = CONFIG_FILE_PATH,\n",
    "                 params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_unit_test_config(self) -> UnitTestConfig:\n",
    "        \"\"\"read training evaluation config file and store as \n",
    "        config entity then apply the dataclasses\n",
    "        \n",
    "        Returns:\n",
    "            config: UnitTestConfig type\n",
    "        \"\"\"\n",
    "        predict_config = self.config.predict\n",
    "        unit_test_config = self.config.unit_test\n",
    "\n",
    "        create_directories([unit_test_config.root_dir])\n",
    "\n",
    "        config = UnitTestConfig(\n",
    "            root_dir=unit_test_config.root_dir,\n",
    "            mlflow_tracking_uri=os.environ[\"MLFLOW_TRACKING_URI\"],\n",
    "            mlflow_model_name=predict_config.mlflow_model_name,\n",
    "            mlflow_deploy_model_alias=os.environ[\"MLFLOW_DEPLOY_MODEL_ALIAS\"],\n",
    "            mlflow_input_example_path=unit_test_config.mlflow_input_example_path,\n",
    "            app_endpoint=os.environ[\"APP_ENDPOINT\"]\n",
    "        )\n",
    "\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.artifacts import download_artifacts\n",
    "from mlflow import MlflowClient\n",
    "from mlflow import pyfunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Debug**: Explain when doing the preparation test in the notebook with MLflow like load input example and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigurationManager()\n",
    "unit_test_config = config.get_unit_test_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the deployed model from MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient(tracking_uri=unit_test_config.mlflow_tracking_uri)\n",
    "selected_model = client.get_model_version_by_alias(\n",
    "    unit_test_config.mlflow_model_name, \n",
    "    unit_test_config.mlflow_deploy_model_alias\n",
    ")\n",
    "\n",
    "selected_model.source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pyfunc.load_model(model_uri=selected_model.source)\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model `run_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_run_id = selected_model.run_id\n",
    "selected_run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_artifacts(\n",
    "    run_id=selected_run_id,\n",
    "    artifact_path=unit_test_config.mlflow_input_example_path,\n",
    "    dst_path=unit_test_config.root_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f = open(f\"{unit_test_config.root_dir}/{unit_test_config.mlflow_input_example_path}\")\n",
    "input_example = json.load(f)\n",
    "input_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the input data from MLflow input examples and try to match with the MLflow input example format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_body = {\n",
    "    input_example[\"columns\"][0]: input_example['data'][0][0]\n",
    "}\n",
    "\n",
    "request_body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the `app.py` with http request with MLflow input data example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "result = requests.post(url=unit_test_config.app_endpoint, json=request_body)\n",
    "y_predict = result.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Unit Testing\n",
    "\n",
    "This code in `src/MarketplaceReviews/components/unit_testing.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "from MarketplaceReviews import logger\n",
    "\n",
    "class UnitTesting:\n",
    "    def __init__(self, config: UnitTestConfig):\n",
    "        self.config = config\n",
    "        self.req_body_key = None\n",
    "        self.req_body = None\n",
    "    \n",
    "    def set_request_body(self) -> None:\n",
    "        \"\"\"predict the data with linear regression model\n",
    "\n",
    "        Raises:\n",
    "            client_error: error when access mlflow to get deployed model\n",
    "            download_error: error when download vectorizer from mlflow artifact\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Set MLflow Client.\")\n",
    "            client = MlflowClient(tracking_uri=self.config.mlflow_tracking_uri)\n",
    "            selected_model = client.get_model_version_by_alias(\n",
    "                self.config.mlflow_model_name, \n",
    "                self.config.mlflow_deploy_model_alias\n",
    "            )\n",
    "            \n",
    "            logger.info(\"Get the deployed model run id.\")\n",
    "            selected_run_id = selected_model.run_id\n",
    "        except Exception as client_error:\n",
    "            logger.error(client_error)\n",
    "            raise client_error\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Downloading vectorizer from MLflow's artifacts.\")\n",
    "            download_artifacts(\n",
    "                run_id=selected_run_id,\n",
    "                artifact_path=self.config.mlflow_input_example_path,\n",
    "                dst_path=self.config.root_dir\n",
    "            )\n",
    "        except Exception as download_error:\n",
    "            logger.error(download_error)\n",
    "            raise download_error\n",
    "        \n",
    "        logger.info(\"Open MLflow input example.\")\n",
    "        f = open(f\"{self.config.root_dir}/{self.config.mlflow_input_example_path}\")\n",
    "        input_example = json.load(f)\n",
    "\n",
    "        # handle mlflow input example data\n",
    "        data_key = input_example[\"columns\"][0]\n",
    "        data_val = input_example['data'][0][0]\n",
    "\n",
    "        # request params\n",
    "        self.req_body_key = data_key\n",
    "        self.req_body = {\n",
    "            data_key: data_val\n",
    "        }\n",
    "        \n",
    "    def get_request_body_value(self) -> list:\n",
    "        \"\"\"get the request body data\n",
    "\n",
    "        Returns:\n",
    "            req_body: list type\n",
    "        \"\"\"\n",
    "        logger.info(\"Get MLflow input example value.\")\n",
    "        req_body_value = self.req_body[self.req_body_key]\n",
    "        return req_body_value\n",
    "    \n",
    "    def get_output_length(self):\n",
    "        \"\"\"get the output length of the predict result\n",
    "\n",
    "        Returns:\n",
    "            len_result: list type\n",
    "        \"\"\"\n",
    "        logger.info(\"Get predicted result length.\")\n",
    "        result = requests.post(\n",
    "            url=self.config.app_endpoint, \n",
    "            json=self.req_body\n",
    "        )\n",
    "        len_result = len(result.json())\n",
    "        return len_result\n",
    "\n",
    "    def is_output_type_list(self) -> bool:\n",
    "        \"\"\"check if the output file is list data type\n",
    "\n",
    "        Returns:\n",
    "            is_list: bool type\n",
    "        \"\"\"\n",
    "        logger.info(\"Check is the predicted output is list.\")\n",
    "        result = requests.post(\n",
    "            url=self.config.app_endpoint, \n",
    "            json=self.req_body\n",
    "        )\n",
    "        is_list = type(result.json()) is list\n",
    "        return is_list\n",
    "\n",
    "    def is_output_type_consistent(self) -> bool:\n",
    "        \"\"\"check if the output file have consistent\n",
    "        data type inside a list\n",
    "\n",
    "        Returns:\n",
    "            bool type\n",
    "        \"\"\"\n",
    "        logger.info(\"Check is each predicted output is integer\")\n",
    "        result = requests.post(\n",
    "            url=self.config.app_endpoint, \n",
    "            json=self.req_body\n",
    "        )\n",
    "        for result in result.json():\n",
    "            if type(result) is not int:\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Testing\n",
    "\n",
    "**Debug**: Simulate the unit testing without library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    unit_testing_config = config.get_unit_test_config()\n",
    "    unit_test = UnitTesting(config=unit_testing_config)\n",
    "    unit_test.set_request_body()\n",
    "    \n",
    "    print(\"Review Contents: \")\n",
    "    for content in unit_test.get_request_body_value():\n",
    "        print(content)\n",
    "    \n",
    "    print(\"\\nBegin tests:\")\n",
    "    print(f\"Is same size: {unit_test.get_output_length() == len(unit_test.get_request_body_value())}\")\n",
    "    print(f\"Is the output is list: {unit_test.is_output_type_list() == True}\")\n",
    "    print(f\"Is the output consistent: {unit_test.is_output_type_consistent() == True}\")\n",
    "except Exception as e:\n",
    "    logger.error(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lazada-id-reviews-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
