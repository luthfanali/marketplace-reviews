{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the main directory\n",
    "# So, it's executed from main directory\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.env') as f:\n",
    "    os.environ.update(\n",
    "        line.strip().split('=') for line in f\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Config\n",
    "\n",
    "This code will be apply in `src/MarketplaceReviews/entity/config_entity.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    input_train_path: Path\n",
    "    input_test_path: Path\n",
    "    output_train_path: Path\n",
    "    output_test_path: Path\n",
    "    vectorized_train_path: Path\n",
    "    vectorized_test_path: Path\n",
    "    vectorizer_model_path: Path\n",
    "    model_path: Path\n",
    "    score_path: Path\n",
    "    mlflow_dataset_path: Path\n",
    "    mlflow_dataset_column: list\n",
    "    minio_endpoint_url: str\n",
    "    minio_access_key_id: str\n",
    "    minio_secret_access_key: str\n",
    "    mlflow_tracking_uri: str\n",
    "    mlflow_exp_name: str\n",
    "    mlflow_dataset_bucket: str\n",
    "    mlflow_run_name: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Config Manager\n",
    "\n",
    "This code will be apply in `src/MarketplaceReviews/config/configurations.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MarketplaceReviews.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "from MarketplaceReviews.utils.common import read_yaml, create_directories, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, \n",
    "                 config_filepath = CONFIG_FILE_PATH,\n",
    "                 params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_train_eval_config(self) -> TrainEvaluationConfig:\n",
    "        \"\"\"read training evaluation config file and store as \n",
    "        config entity then apply the dataclasses\n",
    "        \n",
    "        Returns:\n",
    "            config: TrainEvaluationConfig type\n",
    "        \"\"\"\n",
    "        data_dump_config = self.config.dump_data\n",
    "        vectorize_config = self.config.vectorize_data\n",
    "        train_config = self.config.train_model\n",
    "        eval_config = self.config.train_evaluation\n",
    "\n",
    "        create_directories([eval_config.root_dir])\n",
    "\n",
    "        config = TrainEvaluationConfig(\n",
    "            root_dir=eval_config.root_dir,\n",
    "            input_train_path=Path(data_dump_config.input_train_path),\n",
    "            input_test_path=Path(data_dump_config.input_test_path),\n",
    "            output_train_path=Path(data_dump_config.output_train_path),\n",
    "            output_test_path=Path(data_dump_config.output_test_path),\n",
    "            vectorized_train_path=Path(vectorize_config.vectorized_train_path),\n",
    "            vectorized_test_path=Path(vectorize_config.vectorized_test_path),\n",
    "            vectorizer_model_path=Path(vectorize_config.vectorizer_model_path),\n",
    "            model_path=Path(train_config.model_path),\n",
    "            score_path=Path(eval_config.score_path),\n",
    "            mlflow_dataset_path=Path(eval_config.mlflow_dataset_path),\n",
    "            mlflow_dataset_column=eval_config.mlflow_dataset_column,\n",
    "            minio_endpoint_url=os.environ['MLFLOW_S3_ENDPOINT_URL'],\n",
    "            minio_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "            minio_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],\n",
    "            mlflow_tracking_uri=os.environ[\"MLFLOW_TRACKING_URI\"],\n",
    "            mlflow_exp_name=eval_config.mlflow_exp_name,\n",
    "            mlflow_dataset_bucket=os.environ[\"PROJECT_BUCKET\"],\n",
    "            mlflow_run_name=eval_config.mlflow_run_name\n",
    "        )\n",
    "\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config in `src/MarketplaceReviews/components/model_evaluation.py`\n",
    "\n",
    "Logging is tied with a runs, which is **one cycle training and evaluating model**.\n",
    "To start logging, we have to give mlflow **a context**, which is our **current run**.\n",
    "\n",
    "Steps:\n",
    "+ Load train and test data (text and vectorized), its target data, and the vectorizer.\n",
    "+ Pointing the mlflow client in our program to our mlflow server.\n",
    "+ Set experiment.\n",
    "+ Set MLflow to run and start logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import joblib\n",
    "import mlflow\n",
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from mlflow.data.pandas_dataset import PandasDataset\n",
    "from mlflow.data.dataset_source import DatasetSource\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from MarketplaceReviews import logger\n",
    "\n",
    "class TrainEvaluation:\n",
    "    def __init__(self, config: TrainEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def get_prediction(self, model, X_input_vec, X_input) -> pd.DataFrame:\n",
    "        \"\"\"predict the input data with the model\n",
    "        \n",
    "        Args:\n",
    "            model (Any): the machine learning model\n",
    "            X_input_vec (Any): the vectorized input data\n",
    "            X_input (pd.Series): the input data\n",
    "        \n",
    "        Returns:\n",
    "            pd.Dataframe: prediction result in dataframe\n",
    "        \"\"\"\n",
    "        y_predict = pd.Series(model.predict(X_input_vec), index = X_input.index)\n",
    "        \n",
    "        return y_predict\n",
    "    \n",
    "    def get_report(self, y_output, y_predict) -> dict:\n",
    "        \"\"\"generate the classification report and dump the report as json\n",
    "        \n",
    "        Args:\n",
    "            y_output (pd.Series): the actual output data\n",
    "            y_predict (pd.Series): the prediction result\n",
    "        \n",
    "        Returns:\n",
    "            dict: classification report in dict format\n",
    "        \"\"\"\n",
    "        metrics = classification_report(y_output, y_predict, output_dict=True)\n",
    "        \n",
    "        logger.info(f\"Save report as json.\")\n",
    "        save_json(path=self.config.score_path, data=metrics)\n",
    "        \n",
    "        logger.info(f\"Show the training report.\")\n",
    "        print(f\"\\n{classification_report(y_output, y_predict)}\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def get_mlflow_metrics(self, metrics) -> dict:\n",
    "        \"\"\"generate the classification report for MLflow\n",
    "\n",
    "        Args:\n",
    "            metrics (dict): the classification report\n",
    "        \n",
    "        Returns:\n",
    "            dict: classification report in dict format\n",
    "        \"\"\"\n",
    "        mlflow_metrics = {}\n",
    "\n",
    "        for rating in range(len(metrics) - 3):\n",
    "            data_metric = metrics[str(rating + 1)]\n",
    "            for name, value in data_metric.items():\n",
    "                mlflow_metrics[name + \"_\" + str(rating + 1)] = value\n",
    "        \n",
    "        return mlflow_metrics\n",
    "    \n",
    "    def get_dataset(self, X_input, y_output, y_predict) -> pd.DataFrame:\n",
    "        \"\"\"construct the dataset and save as dataframe and csv file\n",
    "        \n",
    "        Args:\n",
    "            X_input (pd.Series): the input data\n",
    "            y_output (pd.Series): the actual output data\n",
    "            y_predict (pd.Series): the prediction result\n",
    "        \n",
    "        Returns:\n",
    "            pd.Dataframe: prediction result in dataframe\n",
    "        \"\"\"\n",
    "        train_eval_result = pd.concat([X_input, y_output, y_predict], axis = 1)\n",
    "        train_eval_result.columns = self.config.mlflow_dataset_column\n",
    "        train_eval_result.to_csv(self.config.mlflow_dataset_path, index=False)\n",
    "        \n",
    "        return train_eval_result\n",
    "        \n",
    "    def get_mlflow_dataset(self, mlflow_dataset, run_name) -> PandasDataset:\n",
    "        \"\"\"convert the dataset into MLflow's dataset format\n",
    "        \n",
    "        Args:\n",
    "            mlflow_dataset (pd.Series): the project dataset to train and the result\n",
    "            run_name (str): the name of MLflow runs\n",
    "        \n",
    "        Returns:\n",
    "            PandasDataset: the dataset in Pandas MLflow format\n",
    "        \"\"\"\n",
    "        mlflow_dataset: PandasDataset=mlflow.data.from_pandas(\n",
    "            mlflow_dataset,\n",
    "            source=DatasetSource.load(f\"s3://{self.config.mlflow_dataset_bucket}/{run_name}.csv\"),\n",
    "            name=f\"{run_name}\",\n",
    "            targets=self.config.mlflow_dataset_column[1],\n",
    "            predictions=self.config.mlflow_dataset_column[2]\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Remove {self.config.mlflow_dataset_path} file from local.\")\n",
    "        os.remove(self.config.mlflow_dataset_path)\n",
    "        \n",
    "        return mlflow_dataset\n",
    "    \n",
    "    def s3_upload_mlflow_dataset(self, run_name) -> None:\n",
    "        \"\"\"upload the dataset into MinIO with MLflow run_name\n",
    "        \n",
    "        Args:\n",
    "            run_name (str): the name of MLflow runs\n",
    "        \"\"\"\n",
    "        s3 = boto3.client('s3',\n",
    "                              endpoint_url=self.config.minio_endpoint_url,\n",
    "                              aws_access_key_id=self.config.minio_access_key_id,\n",
    "                              aws_secret_access_key=self.config.minio_secret_access_key)\n",
    "        \n",
    "        try:\n",
    "            s3.upload_file(\n",
    "                self.config.mlflow_dataset_path, \n",
    "                self.config.mlflow_dataset_bucket, \n",
    "                f'{run_name}.csv'\n",
    "            )    \n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "            raise e\n",
    "    \n",
    "    def mlflow_log_train(self) -> None:\n",
    "        \"\"\"perform experimentation with MLflow to evaluate the training result\n",
    "        \"\"\"\n",
    "        logger.info(f\"Load vectorized data train from {self.config.vectorized_train_path}.\")\n",
    "        X_train_vec = joblib.load(self.config.vectorized_train_path)\n",
    "        # X_test_vec = joblib.load(self.config.vectorized_test_path)\n",
    "        \n",
    "        logger.info(f\"Load data train from {self.config.input_train_path}.\")\n",
    "        X_train = joblib.load(self.config.input_train_path)\n",
    "        X_test = joblib.load(self.config.input_test_path)\n",
    "        \n",
    "        logger.info(f\"Load data train output from {self.config.output_train_path}.\")\n",
    "        y_train = joblib.load(self.config.output_train_path)\n",
    "        # y_test = joblib.load(self.config.output_test_path)\n",
    "        \n",
    "        logger.info(f\"Load the model.\")\n",
    "        model = joblib.load(self.config.model_path)\n",
    "        \n",
    "        logger.info(f\"Predicting the data train.\")\n",
    "        y_train_pred = self.get_prediction(model, X_train_vec, X_train)\n",
    "        \n",
    "        logger.info(f\"Generate classification report.\")\n",
    "        train_report = self.get_report(y_train, y_train_pred)\n",
    "        \n",
    "        logger.info(f\"Set tracking URI.\")\n",
    "        mlflow.set_tracking_uri(self.config.mlflow_tracking_uri)\n",
    "        \n",
    "        logger.info(f\"Set experiment name.\")\n",
    "        mlflow.set_experiment(self.config.mlflow_exp_name)\n",
    "        \n",
    "        logger.info(f\"Set run name.\")\n",
    "        flag = ''.join(random.choices(\n",
    "            string.ascii_uppercase + string.ascii_lowercase + string.digits, \n",
    "            k=5))\n",
    "        run_name = f\"{self.config.mlflow_run_name}-{flag}\"\n",
    "        \n",
    "        logger.info(f\"Contruct report for MLflow.\")\n",
    "        mlflow_metrics = self.get_mlflow_metrics(train_report)\n",
    "        \n",
    "        logger.info(f\"Contruct MLflow dataset file in {self.config.mlflow_dataset_path}.\")\n",
    "        mlflow_train_dataset = self.get_dataset(X_train, y_train, y_train_pred)\n",
    "\n",
    "        logger.info(f\"Contruct MLflow input example\")\n",
    "        sample = 10\n",
    "        input_example = {\"reviewContents\": X_test.to_list()[:sample]}\n",
    "\n",
    "        logger.info(f\"Experiement tracking to evaluate model with MLflow.\")\n",
    "        with mlflow.start_run(run_name=run_name):\n",
    "            logger.info(f\"Upload {self.config.mlflow_dataset_path} file to MinIO.\")\n",
    "            self.s3_upload_mlflow_dataset(run_name)\n",
    "            \n",
    "            logger.info(f\"Set MLflow dataset.\")\n",
    "            dataset = self.get_mlflow_dataset(mlflow_train_dataset, run_name)\n",
    "\n",
    "            logger.info(f\"Logging to MLflow as an experiment.\")\n",
    "            model_params = model.get_params()\n",
    "            mlflow.log_params(model_params)\n",
    "            mlflow.log_metrics(mlflow_metrics)\n",
    "            mlflow.log_input(dataset, context=\"training\")\n",
    "            mlflow.log_artifact(self.config.vectorizer_model_path, \"vectorizer\")\n",
    "            mlflow.sklearn.log_model(\n",
    "                sk_model=model,\n",
    "                artifact_path=\"models\",\n",
    "                serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_CLOUDPICKLE,\n",
    "                registered_model_name=\"logistic_regression\",\n",
    "                input_example=input_example\n",
    "            )\n",
    "            \n",
    "            mlflow.set_tags(\n",
    "                {\n",
    "                    \"dataset\": \"review contents training dataset and prediction result\",\n",
    "                    \"model\": \"logistic_regression\"\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    eval_config = config.get_train_eval_config()\n",
    "    evaluation = TrainEvaluation(config=eval_config)\n",
    "    evaluation.mlflow_log_train()\n",
    "except Exception as e:\n",
    "    logger.error(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Debug**: Check the dataset in MLflow and MinIO\n",
    "\n",
    "by checking the MLflow last active run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = mlflow.get_run(mlflow.last_active_run().info.run_id)\n",
    "dataset_info = run.inputs.dataset_inputs[0].dataset\n",
    "print(f\"Dataset name: {dataset_info.name}\")\n",
    "print(f\"Dataset digest: {dataset_info.digest}\")\n",
    "print(f\"Dataset profile: {dataset_info.profile}\")\n",
    "print(f\"Dataset schema: {dataset_info.schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    eval_config = config.get_train_eval_config()\n",
    "\n",
    "    s3 = boto3.client('s3',\n",
    "                    endpoint_url=eval_config.minio_endpoint_url,\n",
    "                    aws_access_key_id=eval_config.minio_access_key_id,\n",
    "                    aws_secret_access_key=eval_config.minio_secret_access_key)\n",
    "\n",
    "    obj = s3.get_object(Bucket=eval_config.mlflow_dataset_bucket, Key=f\"{dataset_info.name}.csv\") \n",
    "    df = pd.read_csv(obj['Body'])\n",
    "except Exception as e:\n",
    "    logger.error(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lazada-id-reviews-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
